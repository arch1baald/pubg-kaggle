{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['train_V2.csv', 'test_V2.csv', 'sample_submission_V2.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "84997b837e7028f5a22176f9d0059a0a55acd6d3"
      },
      "cell_type": "markdown",
      "source": "utils.py"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd409f738d39ccf4ea082d5bef12405e74404113"
      },
      "cell_type": "code",
      "source": "import re\nimport bz2\nimport pickle\nfrom datetime import datetime\n\n\ndef camelcase_to_underscore(name):\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n\n\ndef save_model(pipeline_model_index_score):\n    current_datetime = datetime.now().strftime('%d.%m.%Y-%H.%M.%S')\n    str_valid_score = '{0:.5f}'.format(pipeline_model_index_score['valid_score'])\n    name = f'valid_score_{str_valid_score}__{current_datetime}'\n    path = f'models/{name}.pkl.bz2'\n    with bz2.BZ2File(path, 'w') as fout:\n        pickle.dump(pipeline_model_index_score, fout)\n\n\ndef load_model(path):\n    with bz2.BZ2File(path, 'r') as fin:\n        return pickle.load(fin)\n\n\ndef predict_from_file(df, path):\n    model = load_model(path)\n    x = model['pipeline'].transform(df)\n    return model['model'].predict(x)\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb9453fbc216f4375ce135ef52261cdec2398eb4"
      },
      "cell_type": "markdown",
      "source": "features.py"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "730f4e791ebe55107c483e1c44cdb04c9c5222f9"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass FeatureGenerator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Иерархия:\n        - SimpleFeatureGenerator\n        - GroupAggregatedFeatureGenerator,\n    \"\"\"\n    def __init__(self, numerical_columns, id_columns=None, target_column=None, categorical_columns=None):\n        self.created_features = None\n        self.id_columns = id_columns\n        self.target_column = target_column\n        self.categorical_columns = categorical_columns\n        self.numerical_columns = numerical_columns\n\n    def fit_transform(self, df, y=None, **fit_params):\n        return self.transform(df)\n\n    def transform(self, df):\n        print('FeatureGenerator ...')\n        # Hand Written Features\n        simple_feature_generator = SimpleFeatureGenerator()\n        df_features = pd.concat([df, simple_feature_generator.fit_transform(df)], axis=1)\n\n        # 1-st level\n        features = self.numerical_columns + simple_feature_generator.get_feature_names()\n        df_features = pd.concat([\n            df_features,\n            GroupAggregatedFeatureGenerator(features).fit_transform(df_features),\n        ], axis=1)\n\n        if self.created_features is None:\n            self.created_features = [col for col in df_features.columns if col in df.columns]\n        else:\n#             assert self.created_features == [col for col in df_features.columns if col in df.columns]\n            pass\n        return df_features\n\n    def fit(self, x, y=None, **fit_params):\n        return self\n\n    def get_feature_names(self):\n        return self.created_features\n\n\nclass SimpleFeatureGenerator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Based on https://www.kaggle.com/deffro/eda-is-fun\n    \"\"\"\n    def __init__(self):\n        self.created_features = None\n\n    def fit_transform(self, df, y=None, **fit_params):\n        return self.transform(df)\n    \n    def transform(self, df):\n        df_features = pd.DataFrame()\n        df_features['players_joined'] = df.groupby('match_id')['match_id'].transform('count')\n        df_features['total_distance'] = df['ride_distance'] + df['walk_distance'] + df['swim_distance']\n        df_features['kills_norm'] = df['kills'] * ((100 - df_features['players_joined']) / 100 + 1)\n        df_features['damage_dealt_norm'] = df['damage_dealt'] * ((100 - df_features['players_joined']) / 100 + 1)\n        df_features['heals_and_boosts'] = df['heals'] + df['boosts']\n        df_features['total_distance'] = df['walk_distance'] + df['ride_distance'] + df['swim_distance']\n        df_features['boosts_per_walk_distance'] = df['boosts'] / (df['walk_distance'] + 1)\n        df_features['boosts_per_walk_distance'].fillna(0, inplace=True)\n        df_features['heals_per_walk_distance'] = df['heals'] / (df['walk_distance'] + 1)\n        df_features['heals_per_walk_distance'].fillna(0, inplace=True)\n        df_features['heals_and_boosts_per_walk_distance'] = df_features['heals_and_boosts'] / (df['walk_distance'] + 1)\n        df_features['heals_and_boosts_per_walk_distance'].fillna(0, inplace=True)\n        df_features['kills_per_walk_distance'] = df['kills'] / (df['walk_distance'] + 1)\n        df_features['kills_per_walk_distance'].fillna(0, inplace=True)\n        df_features['team'] = [1 if i > 50 else 2 if (bool(i > 25) & bool(i <= 50)) else 4 for i in df['num_groups']]\n        \n        if self.created_features is None:\n            self.created_features = list(df_features.columns)\n        else:\n#             assert self.created_features == list(df_features.columns)\n            pass\n        return df_features\n\n    def fit(self, x, y=None, **fit_params):\n        return self\n    \n    def get_feature_names(self):\n        return self.created_features\n\n\nclass GroupAggregatedFeatureGenerator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Based on https://www.kaggle.com/anycode/simple-nn-baseline-4\n    \"\"\"\n    def __init__(self, features):\n        self.created_features = None\n        self.features = features\n\n    def fit_transform(self, df, y=None, **fit_params):\n        return self.transform(df)\n\n    def transform(self, df):\n        df_features = []\n        # Aggregate by Group\n        for agg_type in ('mean', 'max', 'min'):\n            df_aggregated = df.groupby(['match_id', 'group_id'], as_index=False)[self.features].agg(agg_type)\n            df_aggregated = self.restore_row_order(df, df_aggregated, on=['match_id', 'group_id'])\n            agg_column_names = {col: f'{agg_type}_group_{col}' for col in self.features}\n            df_aggregated.rename(columns=agg_column_names, inplace=True)\n\n            # Rank Groups by Match\n            columns_to_select = list(agg_column_names.values())\n            # Anyway deletes match_id\n#             df_ranked = df_aggregated.groupby('match_id', as_index=False)[columns_to_select].rank(pct=True)\n#             ranked_column_names = {col: f'rank_{col}' for col in columns_to_select}\n#             df_ranked.rename(columns=ranked_column_names, inplace=True)\n            # Unsafe merge because of rank, which deletes match_id\n#             df_aggregated_ranked = pd.concat([df_aggregated, df_ranked], axis=1)\n#             df_features.append(df_aggregated_ranked)\n#             del df_aggregated, df_ranked\n            df_features.append(df_aggregated)\n            del df_aggregated\n        df_features = pd.concat(df_features, axis=1)\n\n        if self.created_features is None:\n            self.created_features = list(df_features.columns)\n        else:\n            if self.created_features == list(df_features.columns):\n                print('Lost features')\n                for col in df_features.columns:\n                    if col not in self.created_features:\n                        print(col)\n        return df_features\n\n    def fit(self, x, y=None, **fit_params):\n        return self\n\n    def get_feature_names(self):\n        return self.created_features\n\n    def restore_row_order(self, df, df_aggregated, on):\n        \"\"\"\n        Восстановление индекса, FeatureUnion просто стакает колонки,\n        поэтому результаты надо приводить к индексу в исходном датафрейме.\n        :param df:\n        :param df_aggregated:\n        :param on:\n        :return:\n        \"\"\"\n        if isinstance(on, list):\n            left_selected = ['index'] + on\n        else:\n            left_selected = ['index', on]\n        df_features = df.reset_index()[left_selected].merge(\n            df_aggregated,\n            how='left',\n            on=on,\n        )\n        df_features.set_index('index', inplace=True)\n        df_features.sort_index(inplace=True)\n        return df_features\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7cca64cc302068279f0951e821e40011799ed393"
      },
      "cell_type": "markdown",
      "source": "preprocessing.py"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c9af11cb66dfc7709d03da80cd7651074643632"
      },
      "cell_type": "code",
      "source": "SELECTED_FEATURES = [\n    'damage_dealt',\n     'dbn_os',\n     'kill_place',\n     'kills',\n     'longest_kill',\n     'match_duration',\n     'max_place',\n     'num_groups',\n     'walk_distance',\n     'kills_norm',\n     'damage_dealt_norm',\n     'kills_per_walk_distance',\n     'mean_group_boosts',\n     'mean_group_damage_dealt',\n     'mean_group_dbn_os',\n     'mean_group_kill_place',\n     'mean_group_kills',\n     'mean_group_kill_streaks',\n     'mean_group_longest_kill',\n     'mean_group_match_duration',\n     'mean_group_max_place',\n     'mean_group_num_groups',\n     'mean_group_walk_distance',\n     'mean_group_total_distance',\n     'mean_group_kills_norm',\n     'mean_group_kills_per_walk_distance',\n     'max_group_damage_dealt',\n     'max_group_dbn_os',\n     'max_group_kill_place',\n     'max_group_kill_streaks',\n     'max_group_longest_kill',\n     'max_group_match_duration',\n     'max_group_max_place',\n     'max_group_num_groups',\n     'max_group_walk_distance',\n     'max_group_kills_norm',\n     'max_group_damage_dealt_norm',\n     'max_group_kills_per_walk_distance',\n     'min_group_dbn_os',\n     'min_group_kill_place',\n     'min_group_kills',\n     'min_group_kill_streaks',\n     'min_group_longest_kill',\n     'min_group_match_duration',\n     'min_group_max_place',\n     'min_group_num_groups',\n     'min_group_walk_distance',\n     'min_group_kills_norm',\n     'min_group_damage_dealt_norm',\n     'min_group_kills_per_walk_distance'\n]",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b2c088dac3922b36773a6bea7ae70308d1817bf"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nclass Preprocessor(BaseEstimator, TransformerMixin):\n    \"\"\"\n    \"\"\"\n    def __init__(self, numerical_columns, id_columns=None, target_column=None, categorical_columns=None):\n        self.features = None\n        self.id_columns = id_columns\n        self.target_column = target_column\n        self.categorical_columns = categorical_columns\n        self.numerical_columns = numerical_columns\n\n        self.imputer = None\n        self.scaler = None\n\n    def fit_transform(self, df, y=None, **fit_params):\n        print('Preprocessor ...')\n        # Drop columns\n        to_drop = [col for col in df.columns if col in self.id_columns + [self.target_column] + self.categorical_columns]\n        x = df.drop(to_drop, axis=1).copy()\n        # Fill missings\n        x.fillna(0, inplace=True)\n        # Feature Selection\n        non_selected = [col for col in x.columns if col not in SELECTED_FEATURES]\n#         non_selected = []\n        x.drop(non_selected, axis=1, inplace=True)\n        # Normilize\n        self.scaler = MinMaxScaler()\n        self.features = x.columns\n        x = x.astype(np.float64)\n        x = pd.DataFrame(self.scaler.fit_transform(x), columns=[col for col in self.features if col in SELECTED_FEATURES])\n        return x\n\n\n    def transform(self, df):\n        print('Preprocessor ...')\n        # Drop columns\n        to_drop = [col for col in df.columns if col in self.id_columns + [self.target_column] + self.categorical_columns]\n        x = df.drop(to_drop, axis=1).copy()\n        # Fill missings\n        x.fillna(0, inplace=True)\n        # Feature Selection\n        non_selected = [col for col in x.columns if col not in SELECTED_FEATURES]\n#         non_selected = []\n        x.drop(non_selected, axis=1, inplace=True)\n        # Normilize\n        x = pd.DataFrame(self.scaler.fit_transform(x.astype(np.float64)), columns=[col for col in self.features if col in SELECTED_FEATURES])\n        return x\n\n    def fit(self, x, y=None, **fit_params):\n        return self\n\n    def get_feature_names(self):\n        return self.features\n",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a19735e878b32d45bc61c9728c448509d42c48d1"
      },
      "cell_type": "markdown",
      "source": "pipeline.py"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65cc88daad2c13693672f46e3f9025cee14e4d15"
      },
      "cell_type": "code",
      "source": "# from sklearn.base import BaseEstimator, TransformerMixin\n\n# from features import FeatureGenerator\n# from preprocessing import Preprocessor\n\n\nclass NotFittedError(Exception):\n    pass\n\n\nclass Pipeline(BaseEstimator, TransformerMixin):\n    \"\"\"\n    \"\"\"\n    def __init__(self, numerical_columns, id_columns=None, target_column=None, categorical_columns=None):\n        self.created_features = None\n        self.id_columns = id_columns\n        self.target_column = target_column\n        self.categorical_columns = categorical_columns\n        self.numerical_columns = numerical_columns\n\n        self.feature_generator = None\n        self.preprocessor = None\n\n    def fit_transform(self, df, y=None, **fit_params):\n        print('Transforming ...')\n        self.feature_generator = FeatureGenerator(\n            id_columns=self.id_columns,\n            numerical_columns=self.numerical_columns,\n            categorical_columns=self.categorical_columns,\n            target_column=self.target_column,\n        )\n        df_features = self.feature_generator.fit_transform(df)\n\n        self.preprocessor = Preprocessor(\n            id_columns=self.id_columns,\n            numerical_columns=self.numerical_columns,\n            categorical_columns=self.categorical_columns,\n            target_column=self.target_column,\n        )\n        x = self.preprocessor.fit_transform(df_features)\n        return x\n\n    def transform(self, df):\n        print('Transforming ...')\n        if self.feature_generator is None:\n            raise NotFittedError(f'feature_generator = {self.feature_generator}')\n        if self.preprocessor is None:\n            raise NotFittedError(f'preprocessor = {self.preprocessor}')\n\n        df_features = self.feature_generator.transform(df)\n        x = self.preprocessor.transform(df_features)\n        return x\n\n    def fit(self, x, y=None, **fit_params):\n        return self\n\n    def get_feature_names(self):\n        return self.created_features\n\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e3eff0f452bb91123e23d24e5b2bcc28e24daae"
      },
      "cell_type": "code",
      "source": "import re\nimport pickle\nfrom IPython.display import display\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f1cf4ab4c97c7e2456d31866a7eb101e3f1141bc"
      },
      "cell_type": "markdown",
      "source": "KernelsFeaturesWithSelection.ipynb"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "### Read Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1584b9a4a880c74388ec045199a1be8be8ae1cd"
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('../input/train_V2.csv')\n# df = pd.read_csv('../input/train_V2.csv', nrows=100000)\ndf.columns = [camelcase_to_underscore(col) for col in df.columns]\ndisplay(df.head(), df.shape, list(df.columns))",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "               id        group_id       ...       win_points  win_place_perc\n0  7f96b2f878858a  4d4b580de459be       ...             1466          0.4444\n1  eef90569b9d03c  684d5656442f9e       ...                0          0.6400\n2  1eaf90ac73de72  6a4a42c3245a74       ...                0          0.7755\n3  4616d365dd2853  a930a9c79cd721       ...                0          0.1667\n4  315c96c26c9aac  de04010b3458dd       ...                0          0.1875\n\n[5 rows x 29 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>group_id</th>\n      <th>match_id</th>\n      <th>assists</th>\n      <th>boosts</th>\n      <th>damage_dealt</th>\n      <th>dbn_os</th>\n      <th>headshot_kills</th>\n      <th>heals</th>\n      <th>kill_place</th>\n      <th>kill_points</th>\n      <th>kills</th>\n      <th>kill_streaks</th>\n      <th>longest_kill</th>\n      <th>match_duration</th>\n      <th>match_type</th>\n      <th>max_place</th>\n      <th>num_groups</th>\n      <th>rank_points</th>\n      <th>revives</th>\n      <th>ride_distance</th>\n      <th>road_kills</th>\n      <th>swim_distance</th>\n      <th>team_kills</th>\n      <th>vehicle_destroys</th>\n      <th>walk_distance</th>\n      <th>weapons_acquired</th>\n      <th>win_points</th>\n      <th>win_place_perc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7f96b2f878858a</td>\n      <td>4d4b580de459be</td>\n      <td>a10357fd1a4a91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>60</td>\n      <td>1241</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>1306</td>\n      <td>squad-fpp</td>\n      <td>28</td>\n      <td>26</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>244.80</td>\n      <td>1</td>\n      <td>1466</td>\n      <td>0.4444</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>eef90569b9d03c</td>\n      <td>684d5656442f9e</td>\n      <td>aeb375fc57110c</td>\n      <td>0</td>\n      <td>0</td>\n      <td>91.47</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>1777</td>\n      <td>squad-fpp</td>\n      <td>26</td>\n      <td>25</td>\n      <td>1484</td>\n      <td>0</td>\n      <td>0.0045</td>\n      <td>0</td>\n      <td>11.04</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1434.00</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0.6400</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1eaf90ac73de72</td>\n      <td>6a4a42c3245a74</td>\n      <td>110163d8bb94ae</td>\n      <td>1</td>\n      <td>0</td>\n      <td>68.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>47</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>1318</td>\n      <td>duo</td>\n      <td>50</td>\n      <td>47</td>\n      <td>1491</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>161.80</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.7755</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4616d365dd2853</td>\n      <td>a930a9c79cd721</td>\n      <td>f1f1f4ef412d7e</td>\n      <td>0</td>\n      <td>0</td>\n      <td>32.90</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>75</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>1436</td>\n      <td>squad-fpp</td>\n      <td>31</td>\n      <td>30</td>\n      <td>1408</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>202.70</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0.1667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>315c96c26c9aac</td>\n      <td>de04010b3458dd</td>\n      <td>6dc8ff871e21e6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>100.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>45</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>58.53</td>\n      <td>1424</td>\n      <td>solo-fpp</td>\n      <td>97</td>\n      <td>95</td>\n      <td>1560</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>49.75</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.1875</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "(4446966, 29)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['id',\n 'group_id',\n 'match_id',\n 'assists',\n 'boosts',\n 'damage_dealt',\n 'dbn_os',\n 'headshot_kills',\n 'heals',\n 'kill_place',\n 'kill_points',\n 'kills',\n 'kill_streaks',\n 'longest_kill',\n 'match_duration',\n 'match_type',\n 'max_place',\n 'num_groups',\n 'rank_points',\n 'revives',\n 'ride_distance',\n 'road_kills',\n 'swim_distance',\n 'team_kills',\n 'vehicle_destroys',\n 'walk_distance',\n 'weapons_acquired',\n 'win_points',\n 'win_place_perc']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "3de135c791d59e5c3abcc80d762553e0bed92493"
      },
      "cell_type": "markdown",
      "source": "### Drop NaN Target"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "101bd61df28ab6dc6f499ad08c3059ff51078f48"
      },
      "cell_type": "code",
      "source": "df.drop(df[df['win_place_perc'].isnull()].index, inplace=True)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ce97ee3abf4e0cc6cf46345ad3dc300c7b1b5895"
      },
      "cell_type": "markdown",
      "source": "### Select Numerical Features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f976ff196c9d74fdb54a00ce170f108db9335dea"
      },
      "cell_type": "code",
      "source": "id_features = ['id', 'group_id', 'match_id']\ncategorical_features = ['match_type', ]\ntarget_feature = 'win_place_perc'\nbase_features = [col for col in df.columns if col not in id_features + categorical_features + [target_feature]] ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ce90b6719c541cc73732273c77ed2f05d13f718b"
      },
      "cell_type": "markdown",
      "source": "### Train 1"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c74b0684d6fad5060923f5c9e57b749ddef318a"
      },
      "cell_type": "code",
      "source": "%%time\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\n\nkfold = KFold(n_splits = 2)\nkfold.get_n_splits(df)\nlog = []\nfor train_index, valid_index in kfold.split(df):\n    step = dict()\n    pipeline = Pipeline(\n        id_columns=id_features, \n        numerical_columns=base_features,\n        categorical_columns=categorical_features,\n        target_column=target_feature,\n    )\n    x_train = pipeline.fit_transform(df.loc[train_index, :])\n    y_train = df.loc[train_index, target_feature]\n    y_train.fillna(0, inplace=True) \n    x_valid = pipeline.transform(df.loc[valid_index, :])\n    y_valid = df.loc[valid_index, target_feature]\n    \n    print('Fitting ...')\n    lgb_train = lgb.Dataset(x_train, y_train)\n    lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n    lgbm_params = dict(\n        objective='regression',\n        metric='mae',\n        n_jobs=-1,\n        verbose=2,\n        learning_rate=0.1,\n        n_estimators=2000,\n    )\n    model = lgb.train(\n        lgbm_params, \n        lgb_train, \n        valid_sets=lgb_eval, \n        early_stopping_rounds=20,\n    )\n#     model.fit(x_train, y_train)\n    step['train_score'] = mean_absolute_error(y_train, model.predict(x_train))\n#     del x_train, y_train\n    \n    step['valid_score'] = mean_absolute_error(y_valid, model.predict(x_valid))\n    step['model'] = model\n    step['pipeline'] = pipeline\n    step['train_index'] = train_index\n    step['valid_index'] = valid_index\n    try:\n        save_model(step)\n    except Exception:\n        print(\"Warning: Couldn't save the model\")\n    print(step['train_score'], step['valid_score'])\n    log.append(step)\n    break",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:1472: FutureWarning: \nPassing list-likes to .loc or [] with any missing label will raise\nKeyError in the future, you can use .reindex() as an alternative.\n\nSee the documentation here:\nhttps://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n  return self._getitem_tuple(key)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Transforming ...\nFeatureGenerator ...\nPreprocessor ...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:1027: FutureWarning: \nPassing list-likes to .loc or [] with any missing label will raise\nKeyError in the future, you can use .reindex() as an alternative.\n\nSee the documentation here:\nhttps://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n  return getattr(section, self.name)[new_key]\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Transforming ...\nFeatureGenerator ...\nPreprocessor ...\nFitting ...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[1]\tvalid_0's l1: 0.472828\nTraining until validation scores don't improve for 20 rounds.\n[2]\tvalid_0's l1: 0.472828\n[3]\tvalid_0's l1: 0.472828\n[4]\tvalid_0's l1: 0.472828\n[5]\tvalid_0's l1: 0.472828\n[6]\tvalid_0's l1: 0.472828\n[7]\tvalid_0's l1: 0.472828\n[8]\tvalid_0's l1: 0.472828\n[9]\tvalid_0's l1: 0.472828\n[10]\tvalid_0's l1: 0.472828\n[11]\tvalid_0's l1: 0.472828\n[12]\tvalid_0's l1: 0.472828\n[13]\tvalid_0's l1: 0.472828\n[14]\tvalid_0's l1: 0.472828\n[15]\tvalid_0's l1: 0.472828\n[16]\tvalid_0's l1: 0.472828\n[17]\tvalid_0's l1: 0.472828\n[18]\tvalid_0's l1: 0.472828\n[19]\tvalid_0's l1: 0.472828\n[20]\tvalid_0's l1: 0.472828\n[21]\tvalid_0's l1: 0.472828\nEarly stopping, best iteration is:\n[1]\tvalid_0's l1: 0.472828\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m    169\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 170\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \"\"\"\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 568\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "6f83bde46bd1fb5723beabbec8cf0859d488508c"
      },
      "cell_type": "code",
      "source": "# lgb_train = lgb.Dataset(x_train, y_train)\n# lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n# lgbm_params = dict(\n#     objective='regression',\n#     metric='mae',\n#     n_jobs=-1,\n#     verbose=2,\n#     learning_rate=0.1,\n#     n_estimators=2000,\n# )\n# model = lgb.train(\n#         lgbm_params, \n#         lgb_train, \n#         valid_sets=lgb_eval, \n#         early_stopping_rounds=20,\n#     )",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[1]\tvalid_0's l1: 0.246634\nTraining until validation scores don't improve for 20 rounds.\n[2]\tvalid_0's l1: 0.228223\n[3]\tvalid_0's l1: 0.212114\n[4]\tvalid_0's l1: 0.196795\n[5]\tvalid_0's l1: 0.183793\n[6]\tvalid_0's l1: 0.172232\n[7]\tvalid_0's l1: 0.161213\n[8]\tvalid_0's l1: 0.151427\n[9]\tvalid_0's l1: 0.143032\n[10]\tvalid_0's l1: 0.135116\n[11]\tvalid_0's l1: 0.128153\n[12]\tvalid_0's l1: 0.121624\n[13]\tvalid_0's l1: 0.116468\n[14]\tvalid_0's l1: 0.111306\n[15]\tvalid_0's l1: 0.10656\n[16]\tvalid_0's l1: 0.102766\n[17]\tvalid_0's l1: 0.0988835\n[18]\tvalid_0's l1: 0.0958594\n[19]\tvalid_0's l1: 0.0931965\n[20]\tvalid_0's l1: 0.0909657\n[21]\tvalid_0's l1: 0.0890002\n[22]\tvalid_0's l1: 0.0874118\n[23]\tvalid_0's l1: 0.0859108\n[24]\tvalid_0's l1: 0.0843731\n[25]\tvalid_0's l1: 0.0831124\n[26]\tvalid_0's l1: 0.0817853\n[27]\tvalid_0's l1: 0.0807551\n[28]\tvalid_0's l1: 0.0798906\n[29]\tvalid_0's l1: 0.0790306\n[30]\tvalid_0's l1: 0.0784385\n[31]\tvalid_0's l1: 0.077812\n[32]\tvalid_0's l1: 0.0769843\n[33]\tvalid_0's l1: 0.0763544\n[34]\tvalid_0's l1: 0.0760807\n[35]\tvalid_0's l1: 0.0755181\n[36]\tvalid_0's l1: 0.0750337\n[37]\tvalid_0's l1: 0.0745197\n[38]\tvalid_0's l1: 0.0741523\n[39]\tvalid_0's l1: 0.0739154\n[40]\tvalid_0's l1: 0.0735537\n[41]\tvalid_0's l1: 0.0732129\n[42]\tvalid_0's l1: 0.0729122\n[43]\tvalid_0's l1: 0.0724994\n[44]\tvalid_0's l1: 0.0722181\n[45]\tvalid_0's l1: 0.0719846\n[46]\tvalid_0's l1: 0.0720335\n[47]\tvalid_0's l1: 0.0718837\n[48]\tvalid_0's l1: 0.0716938\n[49]\tvalid_0's l1: 0.0715515\n[50]\tvalid_0's l1: 0.0714163\n[51]\tvalid_0's l1: 0.0712645\n[52]\tvalid_0's l1: 0.0711483\n[53]\tvalid_0's l1: 0.0710391\n[54]\tvalid_0's l1: 0.0709614\n[55]\tvalid_0's l1: 0.0708469\n[56]\tvalid_0's l1: 0.0706936\n[57]\tvalid_0's l1: 0.0706813\n[58]\tvalid_0's l1: 0.0705801\n[59]\tvalid_0's l1: 0.0705752\n[60]\tvalid_0's l1: 0.070422\n[61]\tvalid_0's l1: 0.0703107\n[62]\tvalid_0's l1: 0.070237\n[63]\tvalid_0's l1: 0.0701726\n[64]\tvalid_0's l1: 0.0701188\n[65]\tvalid_0's l1: 0.069936\n[66]\tvalid_0's l1: 0.0698268\n[67]\tvalid_0's l1: 0.0697882\n[68]\tvalid_0's l1: 0.0696956\n[69]\tvalid_0's l1: 0.0696923\n[70]\tvalid_0's l1: 0.0696634\n[71]\tvalid_0's l1: 0.0695624\n[72]\tvalid_0's l1: 0.0693993\n[73]\tvalid_0's l1: 0.0693413\n[74]\tvalid_0's l1: 0.0692896\n[75]\tvalid_0's l1: 0.0692139\n[76]\tvalid_0's l1: 0.0691539\n[77]\tvalid_0's l1: 0.0690677\n[78]\tvalid_0's l1: 0.0689383\n[79]\tvalid_0's l1: 0.0688534\n[80]\tvalid_0's l1: 0.0687772\n[81]\tvalid_0's l1: 0.0686607\n[82]\tvalid_0's l1: 0.0685756\n[83]\tvalid_0's l1: 0.0684924\n[84]\tvalid_0's l1: 0.0683617\n[85]\tvalid_0's l1: 0.0683144\n[86]\tvalid_0's l1: 0.0682043\n[87]\tvalid_0's l1: 0.0681637\n[88]\tvalid_0's l1: 0.0681272\n[89]\tvalid_0's l1: 0.0681194\n[90]\tvalid_0's l1: 0.0680427\n[91]\tvalid_0's l1: 0.0679054\n[92]\tvalid_0's l1: 0.0677817\n[93]\tvalid_0's l1: 0.0677044\n[94]\tvalid_0's l1: 0.0677076\n[95]\tvalid_0's l1: 0.0676722\n[96]\tvalid_0's l1: 0.0676611\n[97]\tvalid_0's l1: 0.0674958\n[98]\tvalid_0's l1: 0.0674077\n[99]\tvalid_0's l1: 0.0673912\n[100]\tvalid_0's l1: 0.0673588\n[101]\tvalid_0's l1: 0.0673402\n[102]\tvalid_0's l1: 0.0672264\n[103]\tvalid_0's l1: 0.0671907\n[104]\tvalid_0's l1: 0.0671223\n[105]\tvalid_0's l1: 0.0671008\n[106]\tvalid_0's l1: 0.0669856\n[107]\tvalid_0's l1: 0.0669508\n[108]\tvalid_0's l1: 0.0669074\n[109]\tvalid_0's l1: 0.0668451\n[110]\tvalid_0's l1: 0.0668107\n[111]\tvalid_0's l1: 0.0667462\n[112]\tvalid_0's l1: 0.0667205\n[113]\tvalid_0's l1: 0.0666866\n[114]\tvalid_0's l1: 0.0666446\n[115]\tvalid_0's l1: 0.0666215\n[116]\tvalid_0's l1: 0.0665623\n[117]\tvalid_0's l1: 0.066532\n[118]\tvalid_0's l1: 0.0664763\n[119]\tvalid_0's l1: 0.0664576\n[120]\tvalid_0's l1: 0.0663989\n[121]\tvalid_0's l1: 0.0663704\n[122]\tvalid_0's l1: 0.0662943\n[123]\tvalid_0's l1: 0.0662844\n[124]\tvalid_0's l1: 0.0662494\n[125]\tvalid_0's l1: 0.0661818\n[126]\tvalid_0's l1: 0.0661226\n[127]\tvalid_0's l1: 0.0660999\n[128]\tvalid_0's l1: 0.0660651\n[129]\tvalid_0's l1: 0.0660325\n[130]\tvalid_0's l1: 0.0660158\n[131]\tvalid_0's l1: 0.0660098\n[132]\tvalid_0's l1: 0.065986\n[133]\tvalid_0's l1: 0.0659684\n[134]\tvalid_0's l1: 0.0659318\n[135]\tvalid_0's l1: 0.0658803\n[136]\tvalid_0's l1: 0.0658714\n[137]\tvalid_0's l1: 0.0658371\n[138]\tvalid_0's l1: 0.0657932\n[139]\tvalid_0's l1: 0.0657595\n[140]\tvalid_0's l1: 0.0657604\n[141]\tvalid_0's l1: 0.065712\n[142]\tvalid_0's l1: 0.06569\n[143]\tvalid_0's l1: 0.0656447\n[144]\tvalid_0's l1: 0.065646\n[145]\tvalid_0's l1: 0.0656328\n[146]\tvalid_0's l1: 0.0656344\n[147]\tvalid_0's l1: 0.0655562\n[148]\tvalid_0's l1: 0.0655323\n[149]\tvalid_0's l1: 0.0654459\n[150]\tvalid_0's l1: 0.0653394\n[151]\tvalid_0's l1: 0.0653198\n[152]\tvalid_0's l1: 0.0653018\n[153]\tvalid_0's l1: 0.0652789\n[154]\tvalid_0's l1: 0.0652539\n[155]\tvalid_0's l1: 0.0652487\n[156]\tvalid_0's l1: 0.0651817\n[157]\tvalid_0's l1: 0.065204\n[158]\tvalid_0's l1: 0.0652048\n[159]\tvalid_0's l1: 0.0651827\n[160]\tvalid_0's l1: 0.0651813\n[161]\tvalid_0's l1: 0.0651269\n[162]\tvalid_0's l1: 0.0651645\n[163]\tvalid_0's l1: 0.0651597\n[164]\tvalid_0's l1: 0.0651229\n[165]\tvalid_0's l1: 0.0651005\n[166]\tvalid_0's l1: 0.065135\n[167]\tvalid_0's l1: 0.0651177\n[168]\tvalid_0's l1: 0.0651155\n[169]\tvalid_0's l1: 0.0651091\n[170]\tvalid_0's l1: 0.0651035\n[171]\tvalid_0's l1: 0.0650875\n[172]\tvalid_0's l1: 0.0650545\n[173]\tvalid_0's l1: 0.065007\n[174]\tvalid_0's l1: 0.0650114\n[175]\tvalid_0's l1: 0.0649772\n[176]\tvalid_0's l1: 0.0649719\n[177]\tvalid_0's l1: 0.0649572\n[178]\tvalid_0's l1: 0.0649023\n[179]\tvalid_0's l1: 0.0648994\n[180]\tvalid_0's l1: 0.0648584\n[181]\tvalid_0's l1: 0.0648293\n[182]\tvalid_0's l1: 0.0648355\n[183]\tvalid_0's l1: 0.0647965\n[184]\tvalid_0's l1: 0.064788\n[185]\tvalid_0's l1: 0.0647711\n[186]\tvalid_0's l1: 0.0647692\n[187]\tvalid_0's l1: 0.0647482\n[188]\tvalid_0's l1: 0.0647464\n[189]\tvalid_0's l1: 0.0647401\n[190]\tvalid_0's l1: 0.0646813\n[191]\tvalid_0's l1: 0.0646334\n[192]\tvalid_0's l1: 0.0645888\n[193]\tvalid_0's l1: 0.0645785\n[194]\tvalid_0's l1: 0.0645529\n[195]\tvalid_0's l1: 0.0645348\n[196]\tvalid_0's l1: 0.064515\n[197]\tvalid_0's l1: 0.0645122\n[198]\tvalid_0's l1: 0.0644953\n[199]\tvalid_0's l1: 0.0644934\n[200]\tvalid_0's l1: 0.0644876\n[201]\tvalid_0's l1: 0.0644693\n[202]\tvalid_0's l1: 0.0644762\n[203]\tvalid_0's l1: 0.0644676\n[204]\tvalid_0's l1: 0.0644351\n[205]\tvalid_0's l1: 0.064421\n[206]\tvalid_0's l1: 0.0644056\n[207]\tvalid_0's l1: 0.064391\n[208]\tvalid_0's l1: 0.0643771\n[209]\tvalid_0's l1: 0.0643667\n[210]\tvalid_0's l1: 0.0643638\n[211]\tvalid_0's l1: 0.0643347\n[212]\tvalid_0's l1: 0.0642996\n[213]\tvalid_0's l1: 0.0643119\n[214]\tvalid_0's l1: 0.0643154\n[215]\tvalid_0's l1: 0.0643067\n[216]\tvalid_0's l1: 0.0642986\n[217]\tvalid_0's l1: 0.0643205\n[218]\tvalid_0's l1: 0.0643162\n[219]\tvalid_0's l1: 0.064302\n[220]\tvalid_0's l1: 0.0643004\n[221]\tvalid_0's l1: 0.064294\n[222]\tvalid_0's l1: 0.0642852\n[223]\tvalid_0's l1: 0.0642761\n[224]\tvalid_0's l1: 0.0642636\n[225]\tvalid_0's l1: 0.0642612\n[226]\tvalid_0's l1: 0.0642606\n[227]\tvalid_0's l1: 0.0642607\n[228]\tvalid_0's l1: 0.0642055\n[229]\tvalid_0's l1: 0.0641855\n[230]\tvalid_0's l1: 0.0641771\n[231]\tvalid_0's l1: 0.0641598\n[232]\tvalid_0's l1: 0.0641623\n[233]\tvalid_0's l1: 0.064145\n[234]\tvalid_0's l1: 0.0641378\n[235]\tvalid_0's l1: 0.0641289\n[236]\tvalid_0's l1: 0.0641338\n[237]\tvalid_0's l1: 0.0641057\n[238]\tvalid_0's l1: 0.0640968\n[239]\tvalid_0's l1: 0.0640883\n[240]\tvalid_0's l1: 0.0640414\n[241]\tvalid_0's l1: 0.0640348\n[242]\tvalid_0's l1: 0.064025\n[243]\tvalid_0's l1: 0.0640069\n[244]\tvalid_0's l1: 0.0639995\n[245]\tvalid_0's l1: 0.0639943\n[246]\tvalid_0's l1: 0.0639882\n[247]\tvalid_0's l1: 0.0639892\n[248]\tvalid_0's l1: 0.0640042\n[249]\tvalid_0's l1: 0.0640024\n[250]\tvalid_0's l1: 0.0640054\n[251]\tvalid_0's l1: 0.0640026\n[252]\tvalid_0's l1: 0.0639505\n[253]\tvalid_0's l1: 0.0639501\n[254]\tvalid_0's l1: 0.0639379\n[255]\tvalid_0's l1: 0.0639411\n[256]\tvalid_0's l1: 0.0639495\n[257]\tvalid_0's l1: 0.0639445\n[258]\tvalid_0's l1: 0.0639464\n[259]\tvalid_0's l1: 0.0638908\n[260]\tvalid_0's l1: 0.0638628\n[261]\tvalid_0's l1: 0.063853\n[262]\tvalid_0's l1: 0.0638477\n[263]\tvalid_0's l1: 0.0638417\n[264]\tvalid_0's l1: 0.0638263\n[265]\tvalid_0's l1: 0.0637296\n[266]\tvalid_0's l1: 0.0636626\n[267]\tvalid_0's l1: 0.063634\n[268]\tvalid_0's l1: 0.0636282\n[269]\tvalid_0's l1: 0.0636282\n[270]\tvalid_0's l1: 0.0636308\n[271]\tvalid_0's l1: 0.0636201\n[272]\tvalid_0's l1: 0.0636175\n[273]\tvalid_0's l1: 0.0636022\n[274]\tvalid_0's l1: 0.063538\n[275]\tvalid_0's l1: 0.0635247\n[276]\tvalid_0's l1: 0.0635211\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[277]\tvalid_0's l1: 0.0635096\n[278]\tvalid_0's l1: 0.063507\n[279]\tvalid_0's l1: 0.0635007\n[280]\tvalid_0's l1: 0.0634964\n[281]\tvalid_0's l1: 0.0634852\n[282]\tvalid_0's l1: 0.0634626\n[283]\tvalid_0's l1: 0.063457\n[284]\tvalid_0's l1: 0.0634538\n[285]\tvalid_0's l1: 0.0634407\n[286]\tvalid_0's l1: 0.0634404\n[287]\tvalid_0's l1: 0.0634303\n[288]\tvalid_0's l1: 0.0634273\n[289]\tvalid_0's l1: 0.0634189\n[290]\tvalid_0's l1: 0.0634187\n[291]\tvalid_0's l1: 0.0634216\n[292]\tvalid_0's l1: 0.0634172\n[293]\tvalid_0's l1: 0.0634101\n[294]\tvalid_0's l1: 0.0633905\n[295]\tvalid_0's l1: 0.0633892\n[296]\tvalid_0's l1: 0.0633706\n[297]\tvalid_0's l1: 0.0633057\n[298]\tvalid_0's l1: 0.0632576\n[299]\tvalid_0's l1: 0.0632535\n[300]\tvalid_0's l1: 0.0632501\n[301]\tvalid_0's l1: 0.0632484\n[302]\tvalid_0's l1: 0.0632356\n[303]\tvalid_0's l1: 0.0632295\n[304]\tvalid_0's l1: 0.0632167\n[305]\tvalid_0's l1: 0.0632188\n[306]\tvalid_0's l1: 0.0632059\n[307]\tvalid_0's l1: 0.0631845\n[308]\tvalid_0's l1: 0.0631836\n[309]\tvalid_0's l1: 0.063177\n[310]\tvalid_0's l1: 0.0631817\n[311]\tvalid_0's l1: 0.0631765\n[312]\tvalid_0's l1: 0.0631853\n[313]\tvalid_0's l1: 0.0631892\n[314]\tvalid_0's l1: 0.0631779\n[315]\tvalid_0's l1: 0.0631782\n[316]\tvalid_0's l1: 0.0631554\n[317]\tvalid_0's l1: 0.0631519\n[318]\tvalid_0's l1: 0.0631469\n[319]\tvalid_0's l1: 0.0631448\n[320]\tvalid_0's l1: 0.0631417\n[321]\tvalid_0's l1: 0.0631404\n[322]\tvalid_0's l1: 0.0631393\n[323]\tvalid_0's l1: 0.0631324\n[324]\tvalid_0's l1: 0.0631359\n[325]\tvalid_0's l1: 0.0631327\n[326]\tvalid_0's l1: 0.0631333\n[327]\tvalid_0's l1: 0.063121\n[328]\tvalid_0's l1: 0.0631222\n[329]\tvalid_0's l1: 0.0631156\n[330]\tvalid_0's l1: 0.0630697\n[331]\tvalid_0's l1: 0.0630677\n[332]\tvalid_0's l1: 0.0630765\n[333]\tvalid_0's l1: 0.0630448\n[334]\tvalid_0's l1: 0.0629968\n[335]\tvalid_0's l1: 0.0629879\n[336]\tvalid_0's l1: 0.0629848\n[337]\tvalid_0's l1: 0.0629838\n[338]\tvalid_0's l1: 0.0629707\n[339]\tvalid_0's l1: 0.0629679\n[340]\tvalid_0's l1: 0.0629265\n[341]\tvalid_0's l1: 0.0629216\n[342]\tvalid_0's l1: 0.062913\n[343]\tvalid_0's l1: 0.0629165\n[344]\tvalid_0's l1: 0.0629127\n[345]\tvalid_0's l1: 0.0629021\n[346]\tvalid_0's l1: 0.0628692\n[347]\tvalid_0's l1: 0.0628607\n[348]\tvalid_0's l1: 0.0628602\n[349]\tvalid_0's l1: 0.0628528\n[350]\tvalid_0's l1: 0.0628541\n[351]\tvalid_0's l1: 0.0628574\n[352]\tvalid_0's l1: 0.0628555\n[353]\tvalid_0's l1: 0.0628262\n[354]\tvalid_0's l1: 0.0628246\n[355]\tvalid_0's l1: 0.0628139\n[356]\tvalid_0's l1: 0.0627996\n[357]\tvalid_0's l1: 0.0627943\n[358]\tvalid_0's l1: 0.062798\n[359]\tvalid_0's l1: 0.0627857\n[360]\tvalid_0's l1: 0.0627865\n[361]\tvalid_0's l1: 0.0627573\n[362]\tvalid_0's l1: 0.0627585\n[363]\tvalid_0's l1: 0.0627068\n[364]\tvalid_0's l1: 0.0626989\n[365]\tvalid_0's l1: 0.0626978\n[366]\tvalid_0's l1: 0.062703\n[367]\tvalid_0's l1: 0.0626993\n[368]\tvalid_0's l1: 0.062699\n[369]\tvalid_0's l1: 0.0626911\n[370]\tvalid_0's l1: 0.0626881\n[371]\tvalid_0's l1: 0.062674\n[372]\tvalid_0's l1: 0.0626723\n[373]\tvalid_0's l1: 0.0626747\n[374]\tvalid_0's l1: 0.0626682\n[375]\tvalid_0's l1: 0.0626638\n[376]\tvalid_0's l1: 0.0626578\n[377]\tvalid_0's l1: 0.0626488\n[378]\tvalid_0's l1: 0.0626402\n[379]\tvalid_0's l1: 0.0626385\n[380]\tvalid_0's l1: 0.0626339\n[381]\tvalid_0's l1: 0.0626336\n[382]\tvalid_0's l1: 0.0626342\n[383]\tvalid_0's l1: 0.0626232\n[384]\tvalid_0's l1: 0.0626317\n[385]\tvalid_0's l1: 0.0626054\n[386]\tvalid_0's l1: 0.0626028\n[387]\tvalid_0's l1: 0.0625908\n[388]\tvalid_0's l1: 0.0625839\n[389]\tvalid_0's l1: 0.062579\n[390]\tvalid_0's l1: 0.0625295\n[391]\tvalid_0's l1: 0.062532\n[392]\tvalid_0's l1: 0.0625215\n[393]\tvalid_0's l1: 0.06252\n[394]\tvalid_0's l1: 0.0625196\n[395]\tvalid_0's l1: 0.0625208\n[396]\tvalid_0's l1: 0.0625205\n[397]\tvalid_0's l1: 0.0624854\n[398]\tvalid_0's l1: 0.0624856\n[399]\tvalid_0's l1: 0.0624713\n[400]\tvalid_0's l1: 0.0624704\n[401]\tvalid_0's l1: 0.0624696\n[402]\tvalid_0's l1: 0.06246\n[403]\tvalid_0's l1: 0.0624547\n[404]\tvalid_0's l1: 0.0624555\n[405]\tvalid_0's l1: 0.0624539\n[406]\tvalid_0's l1: 0.0624524\n[407]\tvalid_0's l1: 0.0624492\n[408]\tvalid_0's l1: 0.0624481\n[409]\tvalid_0's l1: 0.0624452\n[410]\tvalid_0's l1: 0.0624455\n[411]\tvalid_0's l1: 0.062439\n[412]\tvalid_0's l1: 0.0624386\n[413]\tvalid_0's l1: 0.0624383\n[414]\tvalid_0's l1: 0.0624241\n[415]\tvalid_0's l1: 0.0624216\n[416]\tvalid_0's l1: 0.0623876\n[417]\tvalid_0's l1: 0.0623866\n[418]\tvalid_0's l1: 0.0623774\n[419]\tvalid_0's l1: 0.062377\n[420]\tvalid_0's l1: 0.0623702\n[421]\tvalid_0's l1: 0.0623424\n[422]\tvalid_0's l1: 0.0623457\n[423]\tvalid_0's l1: 0.0623366\n[424]\tvalid_0's l1: 0.0623369\n[425]\tvalid_0's l1: 0.0623349\n[426]\tvalid_0's l1: 0.062333\n[427]\tvalid_0's l1: 0.0623326\n[428]\tvalid_0's l1: 0.062335\n[429]\tvalid_0's l1: 0.0623234\n[430]\tvalid_0's l1: 0.0623011\n[431]\tvalid_0's l1: 0.0623003\n[432]\tvalid_0's l1: 0.0622753\n[433]\tvalid_0's l1: 0.0622688\n[434]\tvalid_0's l1: 0.062262\n[435]\tvalid_0's l1: 0.062252\n[436]\tvalid_0's l1: 0.0622462\n[437]\tvalid_0's l1: 0.0622496\n[438]\tvalid_0's l1: 0.0623171\n[439]\tvalid_0's l1: 0.0623168\n[440]\tvalid_0's l1: 0.0623136\n[441]\tvalid_0's l1: 0.0623158\n[442]\tvalid_0's l1: 0.0623171\n[443]\tvalid_0's l1: 0.0623126\n[444]\tvalid_0's l1: 0.0623104\n[445]\tvalid_0's l1: 0.0623099\n[446]\tvalid_0's l1: 0.0623096\n[447]\tvalid_0's l1: 0.0623046\n[448]\tvalid_0's l1: 0.0623003\n[449]\tvalid_0's l1: 0.062301\n[450]\tvalid_0's l1: 0.0623056\n[451]\tvalid_0's l1: 0.0623046\n[452]\tvalid_0's l1: 0.0622957\n[453]\tvalid_0's l1: 0.0622963\n[454]\tvalid_0's l1: 0.0622977\n[455]\tvalid_0's l1: 0.0622752\n[456]\tvalid_0's l1: 0.0622739\nEarly stopping, best iteration is:\n[436]\tvalid_0's l1: 0.0622462\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "857837c6124fcaee2dda1afdbb18a8a06181ada8"
      },
      "cell_type": "markdown",
      "source": "### Submission"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6343b903d6f30056791ed04ca1c36a162366c55c"
      },
      "cell_type": "code",
      "source": "df_test = pd.read_csv('../input/test_V2.csv')\ndf_test.columns = [camelcase_to_underscore(col) for col in df_test.columns]",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49ddb860bec139fb4b0415628d83ad3e312c5f7f"
      },
      "cell_type": "code",
      "source": "x_test = pipeline.transform(df_test)",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Transforming ...\nFeatureGenerator ...\nPreprocessor ...\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2ff446657c00a725c89edb3304cd17930432410"
      },
      "cell_type": "code",
      "source": "pred = model.predict(x_test)",
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24d1992032a36575b2404e815aa708abed4011b8"
      },
      "cell_type": "code",
      "source": "df_sub = pd.read_csv('../input/sample_submission_V2.csv')\ndf_sub['winPlacePerc'] = pred\ndf_sub.to_csv('lgb_submission.csv', index=False)",
      "execution_count": 50,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f345bf91b6973a41452f2a877c908aeac7e1e632"
      },
      "cell_type": "code",
      "source": "# Restore some columns\ndf_sub = pd.read_csv('../input/sample_submission_V2.csv')\ndf_sub['winPlacePerc'] = pred\ndf_sub['id'] = df_sub['Id']\ndf_sub = df_sub.merge(df_test[[\"id\", \"match_id\", \"group_id\", \"max_place\", \"num_groups\"]], on=\"id\", how=\"left\")\n\n# Sort, rank, and assign adjusted ratio\ndf_sub_group = df_sub.groupby([\"match_id\", \"group_id\"]).first().reset_index()\ndf_sub_group[\"rank\"] = df_sub_group.groupby([\"match_id\"])[\"winPlacePerc\"].rank()\ndf_sub_group = df_sub_group.merge(\n    df_sub_group.groupby(\"match_id\")[\"rank\"].max().to_frame(\"max_rank\").reset_index(), \n    on=\"match_id\", how=\"left\")\ndf_sub_group[\"adjusted_perc\"] = (df_sub_group[\"rank\"] - 1) / (df_sub_group[\"num_groups\"] - 1)\n\ndf_sub = df_sub.merge(df_sub_group[[\"adjusted_perc\", \"match_id\", \"group_id\"]], on=[\"match_id\", \"group_id\"], how=\"left\")\ndf_sub[\"winPlacePerc\"] = df_sub[\"adjusted_perc\"]\n\n# Deal with edge cases\ndf_sub.loc[df_sub['max_place'] == 0, \"winPlacePerc\"] = 0\ndf_sub.loc[df_sub['max_place'] == 1, \"winPlacePerc\"] = 1\n\n# Align with maxPlace\n# Credit: https://www.kaggle.com/anycode/simple-nn-baseline-4\nsubset = df_sub.loc[df_sub['max_place'] > 1]\ngap = 1.0 / (subset['max_place'].values - 1)\nnew_perc = np.around(subset['winPlacePerc'].values / gap) * gap\ndf_sub.loc[df_sub['max_place'] > 1, \"winPlacePerc\"] = new_perc\n\n# Edge case\ndf_sub.loc[(df_sub['max_place'] > 1) & (df_sub['num_groups'] == 1), \"winPlacePerc\"] = 0\nassert df_sub[\"winPlacePerc\"].isnull().sum() == 0\n\ndf_sub[[\"Id\", \"winPlacePerc\"]].to_csv(\"submission_adjusted.csv\", index=False)",
      "execution_count": 57,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74a56f4f7ff9eea74273a232f9068482d3e9a489"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}