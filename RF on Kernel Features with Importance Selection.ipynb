{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['train_V2.csv', 'test_V2.csv', 'sample_submission_V2.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "84997b837e7028f5a22176f9d0059a0a55acd6d3"
      },
      "cell_type": "markdown",
      "source": "utils.py"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd409f738d39ccf4ea082d5bef12405e74404113"
      },
      "cell_type": "code",
      "source": "import re\nimport bz2\nimport pickle\nfrom datetime import datetime\n\n\ndef camelcase_to_underscore(name):\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n\n\ndef save_model(pipeline_model_index_score):\n    current_datetime = datetime.now().strftime('%d.%m.%Y-%H.%M.%S')\n    str_valid_score = '{0:.5f}'.format(pipeline_model_index_score['valid_score'])\n    name = f'valid_score_{str_valid_score}__{current_datetime}'\n    path = f'models/{name}.pkl.bz2'\n    with bz2.BZ2File(path, 'w') as fout:\n        pickle.dump(pipeline_model_index_score, fout)\n\n\ndef load_model(path):\n    with bz2.BZ2File(path, 'r') as fin:\n        return pickle.load(fin)\n\n\ndef predict_from_file(df, path):\n    model = load_model(path)\n    x = model['pipeline'].transform(df)\n    return model['model'].predict(x)\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb9453fbc216f4375ce135ef52261cdec2398eb4"
      },
      "cell_type": "markdown",
      "source": "features.py"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "730f4e791ebe55107c483e1c44cdb04c9c5222f9"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass FeatureGenerator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Иерархия:\n        - SimpleFeatureGenerator\n        - GroupAggregatedFeatureGenerator,\n    \"\"\"\n    def __init__(self, numerical_columns, id_columns=None, target_column=None, categorical_columns=None):\n        self.created_features = None\n        self.id_columns = id_columns\n        self.target_column = target_column\n        self.categorical_columns = categorical_columns\n        self.numerical_columns = numerical_columns\n\n    def fit_transform(self, df, y=None, **fit_params):\n        return self.transform(df)\n\n    def transform(self, df):\n        print('FeatureGenerator ...')\n        # Hand Written Features\n        simple_feature_generator = SimpleFeatureGenerator()\n        df_features = pd.concat([df, simple_feature_generator.fit_transform(df)], axis=1)\n\n        # 1-st level\n        features = self.numerical_columns + simple_feature_generator.get_feature_names()\n        df_features = pd.concat([\n            df_features,\n            GroupAggregatedFeatureGenerator(features).fit_transform(df_features),\n        ], axis=1)\n\n        if self.created_features is None:\n            self.created_features = [col for col in df_features.columns if col in df.columns]\n        else:\n#             assert self.created_features == [col for col in df_features.columns if col in df.columns]\n            pass\n        return df_features\n\n    def fit(self, x, y=None, **fit_params):\n        return self\n\n    def get_feature_names(self):\n        return self.created_features\n\n\nclass SimpleFeatureGenerator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Based on https://www.kaggle.com/deffro/eda-is-fun\n    \"\"\"\n    def __init__(self):\n        self.created_features = None\n\n    def fit_transform(self, df, y=None, **fit_params):\n        return self.transform(df)\n    \n    def transform(self, df):\n        df_features = pd.DataFrame()\n        df_features['players_joined'] = df.groupby('match_id')['match_id'].transform('count')\n        df_features['total_distance'] = df['ride_distance'] + df['walk_distance'] + df['swim_distance']\n        df_features['kills_norm'] = df['kills'] * ((100 - df_features['players_joined']) / 100 + 1)\n        df_features['damage_dealt_norm'] = df['damage_dealt'] * ((100 - df_features['players_joined']) / 100 + 1)\n        df_features['heals_and_boosts'] = df['heals'] + df['boosts']\n        df_features['total_distance'] = df['walk_distance'] + df['ride_distance'] + df['swim_distance']\n        df_features['boosts_per_walk_distance'] = df['boosts'] / (df['walk_distance'] + 1)\n        df_features['boosts_per_walk_distance'].fillna(0, inplace=True)\n        df_features['heals_per_walk_distance'] = df['heals'] / (df['walk_distance'] + 1)\n        df_features['heals_per_walk_distance'].fillna(0, inplace=True)\n        df_features['heals_and_boosts_per_walk_distance'] = df_features['heals_and_boosts'] / (df['walk_distance'] + 1)\n        df_features['heals_and_boosts_per_walk_distance'].fillna(0, inplace=True)\n        df_features['kills_per_walk_distance'] = df['kills'] / (df['walk_distance'] + 1)\n        df_features['kills_per_walk_distance'].fillna(0, inplace=True)\n        df_features['team'] = [1 if i > 50 else 2 if (bool(i > 25) & bool(i <= 50)) else 4 for i in df['num_groups']]\n        \n        if self.created_features is None:\n            self.created_features = list(df_features.columns)\n        else:\n#             assert self.created_features == list(df_features.columns)\n            pass\n        return df_features\n\n    def fit(self, x, y=None, **fit_params):\n        return self\n    \n    def get_feature_names(self):\n        return self.created_features\n\n\nclass GroupAggregatedFeatureGenerator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Based on https://www.kaggle.com/anycode/simple-nn-baseline-4\n    \"\"\"\n    def __init__(self, features):\n        self.created_features = None\n        self.features = features\n\n    def fit_transform(self, df, y=None, **fit_params):\n        return self.transform(df)\n\n    def transform(self, df):\n        df_features = []\n        # Aggregate by Group\n        for agg_type in ('mean', 'max', 'min'):\n            df_aggregated = df.groupby(['match_id', 'group_id'], as_index=False)[self.features].agg(agg_type)\n            df_aggregated = self.restore_row_order(df, df_aggregated, on=['match_id', 'group_id'])\n            agg_column_names = {col: f'{agg_type}_group_{col}' for col in self.features}\n            df_aggregated.rename(columns=agg_column_names, inplace=True)\n\n            # Rank Groups by Match\n            columns_to_select = list(agg_column_names.values())\n            # Anyway deletes match_id\n#             df_ranked = df_aggregated.groupby('match_id', as_index=False)[columns_to_select].rank(pct=True)\n#             ranked_column_names = {col: f'rank_{col}' for col in columns_to_select}\n#             df_ranked.rename(columns=ranked_column_names, inplace=True)\n            # Unsafe merge because of rank, which deletes match_id\n#             df_aggregated_ranked = pd.concat([df_aggregated, df_ranked], axis=1)\n#             df_features.append(df_aggregated_ranked)\n#             del df_aggregated, df_ranked\n            df_features.append(df_aggregated)\n            del df_aggregated\n        df_features = pd.concat(df_features, axis=1)\n\n        if self.created_features is None:\n            self.created_features = list(df_features.columns)\n        else:\n            if self.created_features == list(df_features.columns):\n                print('Lost features')\n                for col in df_features.columns:\n                    if col not in self.created_features:\n                        print(col)\n        return df_features\n\n    def fit(self, x, y=None, **fit_params):\n        return self\n\n    def get_feature_names(self):\n        return self.created_features\n\n    def restore_row_order(self, df, df_aggregated, on):\n        \"\"\"\n        Восстановление индекса, FeatureUnion просто стакает колонки,\n        поэтому результаты надо приводить к индексу в исходном датафрейме.\n        :param df:\n        :param df_aggregated:\n        :param on:\n        :return:\n        \"\"\"\n        if isinstance(on, list):\n            left_selected = ['index'] + on\n        else:\n            left_selected = ['index', on]\n        df_features = df.reset_index()[left_selected].merge(\n            df_aggregated,\n            how='left',\n            on=on,\n        )\n        df_features.set_index('index', inplace=True)\n        df_features.sort_index(inplace=True)\n        return df_features\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7cca64cc302068279f0951e821e40011799ed393"
      },
      "cell_type": "markdown",
      "source": "preprocessing.py"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c9af11cb66dfc7709d03da80cd7651074643632"
      },
      "cell_type": "code",
      "source": "SELECTED_FEATURES = [\n    'damage_dealt',\n     'dbn_os',\n     'kill_place',\n     'kills',\n     'longest_kill',\n     'match_duration',\n     'max_place',\n     'num_groups',\n     'walk_distance',\n     'kills_norm',\n     'damage_dealt_norm',\n     'kills_per_walk_distance',\n     'mean_group_boosts',\n     'mean_group_damage_dealt',\n     'mean_group_dbn_os',\n     'mean_group_kill_place',\n     'mean_group_kills',\n     'mean_group_kill_streaks',\n     'mean_group_longest_kill',\n     'mean_group_match_duration',\n     'mean_group_max_place',\n     'mean_group_num_groups',\n     'mean_group_walk_distance',\n     'mean_group_total_distance',\n     'mean_group_kills_norm',\n     'mean_group_kills_per_walk_distance',\n     'max_group_damage_dealt',\n     'max_group_dbn_os',\n     'max_group_kill_place',\n     'max_group_kill_streaks',\n     'max_group_longest_kill',\n     'max_group_match_duration',\n     'max_group_max_place',\n     'max_group_num_groups',\n     'max_group_walk_distance',\n     'max_group_kills_norm',\n     'max_group_damage_dealt_norm',\n     'max_group_kills_per_walk_distance',\n     'min_group_dbn_os',\n     'min_group_kill_place',\n     'min_group_kills',\n     'min_group_kill_streaks',\n     'min_group_longest_kill',\n     'min_group_match_duration',\n     'min_group_max_place',\n     'min_group_num_groups',\n     'min_group_walk_distance',\n     'min_group_kills_norm',\n     'min_group_damage_dealt_norm',\n     'min_group_kills_per_walk_distance'\n]",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b2c088dac3922b36773a6bea7ae70308d1817bf"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nclass Preprocessor(BaseEstimator, TransformerMixin):\n    \"\"\"\n    \"\"\"\n    def __init__(self, numerical_columns, id_columns=None, target_column=None, categorical_columns=None):\n        self.features = None\n        self.id_columns = id_columns\n        self.target_column = target_column\n        self.categorical_columns = categorical_columns\n        self.numerical_columns = numerical_columns\n\n        self.imputer = None\n        self.scaler = None\n\n    def fit_transform(self, df, y=None, **fit_params):\n        print('Preprocessor ...')\n        # Drop columns\n        to_drop = [col for col in df.columns if col in self.id_columns + [self.target_column] + self.categorical_columns]\n        x = df.drop(to_drop, axis=1).copy()\n        # Fill missings\n        x.fillna(0, inplace=True)\n        # Feature Selection\n        non_selected = [col for col in x.columns if col not in SELECTED_FEATURES]\n        x.drop(non_selected, axis=1, inplace=True)\n        # Normilize\n        self.scaler = MinMaxScaler()\n        self.features = x.columns\n        x = x.astype(np.float64)\n        x = pd.DataFrame(self.scaler.fit_transform(x), columns=[col for col in self.features if col in SELECTED_FEATURES])\n        return x\n\n\n    def transform(self, df):\n        print('Preprocessor ...')\n        # Drop columns\n        to_drop = [col for col in df.columns if col in self.id_columns + [self.target_column] + self.categorical_columns]\n        x = df.drop(to_drop, axis=1).copy()\n        # Fill missings\n        x.fillna(0, inplace=True)\n        # Feature Selection\n        non_selected = [col for col in x.columns if col not in SELECTED_FEATURES]\n        x.drop(non_selected, axis=1, inplace=True)\n        # Normilize\n        x = pd.DataFrame(self.scaler.fit_transform(x.astype(np.float64)), columns=[col for col in self.features if col in SELECTED_FEATURES])\n        return x\n\n    def fit(self, x, y=None, **fit_params):\n        return self\n\n    def get_feature_names(self):\n        return self.features\n",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a19735e878b32d45bc61c9728c448509d42c48d1"
      },
      "cell_type": "markdown",
      "source": "pipeline.py"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65cc88daad2c13693672f46e3f9025cee14e4d15"
      },
      "cell_type": "code",
      "source": "# from sklearn.base import BaseEstimator, TransformerMixin\n\n# from features import FeatureGenerator\n# from preprocessing import Preprocessor\n\n\nclass NotFittedError(Exception):\n    pass\n\n\nclass Pipeline(BaseEstimator, TransformerMixin):\n    \"\"\"\n    \"\"\"\n    def __init__(self, numerical_columns, id_columns=None, target_column=None, categorical_columns=None):\n        self.created_features = None\n        self.id_columns = id_columns\n        self.target_column = target_column\n        self.categorical_columns = categorical_columns\n        self.numerical_columns = numerical_columns\n\n        self.feature_generator = None\n        self.preprocessor = None\n\n    def fit_transform(self, df, y=None, **fit_params):\n        print('Transforming ...')\n        self.feature_generator = FeatureGenerator(\n            id_columns=self.id_columns,\n            numerical_columns=self.numerical_columns,\n            categorical_columns=self.categorical_columns,\n            target_column=self.target_column,\n        )\n        df_features = self.feature_generator.fit_transform(df)\n\n        self.preprocessor = Preprocessor(\n            id_columns=self.id_columns,\n            numerical_columns=self.numerical_columns,\n            categorical_columns=self.categorical_columns,\n            target_column=self.target_column,\n        )\n        x = self.preprocessor.fit_transform(df_features)\n        return x\n\n    def transform(self, df):\n        print('Transforming ...')\n        if self.feature_generator is None:\n            raise NotFittedError(f'feature_generator = {self.feature_generator}')\n        if self.preprocessor is None:\n            raise NotFittedError(f'preprocessor = {self.preprocessor}')\n\n        df_features = self.feature_generator.transform(df)\n        x = self.preprocessor.transform(df_features)\n        return x\n\n    def fit(self, x, y=None, **fit_params):\n        return self\n\n    def get_feature_names(self):\n        return self.created_features\n\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e3eff0f452bb91123e23d24e5b2bcc28e24daae"
      },
      "cell_type": "code",
      "source": "import re\nimport pickle\nfrom IPython.display import display\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f1cf4ab4c97c7e2456d31866a7eb101e3f1141bc"
      },
      "cell_type": "markdown",
      "source": "KernelsFeaturesWithSelection.ipynb"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "### Read Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1584b9a4a880c74388ec045199a1be8be8ae1cd"
      },
      "cell_type": "code",
      "source": "# df = pd.read_csv('../input/train_V2.csv')\ndf = pd.read_csv('../input/train_V2.csv', nrows=100000)\ndf.columns = [camelcase_to_underscore(col) for col in df.columns]\ndisplay(df.head(), df.shape, list(df.columns))",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "               id        group_id       ...       win_points  win_place_perc\n0  7f96b2f878858a  4d4b580de459be       ...             1466          0.4444\n1  eef90569b9d03c  684d5656442f9e       ...                0          0.6400\n2  1eaf90ac73de72  6a4a42c3245a74       ...                0          0.7755\n3  4616d365dd2853  a930a9c79cd721       ...                0          0.1667\n4  315c96c26c9aac  de04010b3458dd       ...                0          0.1875\n\n[5 rows x 29 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>group_id</th>\n      <th>match_id</th>\n      <th>assists</th>\n      <th>boosts</th>\n      <th>damage_dealt</th>\n      <th>dbn_os</th>\n      <th>headshot_kills</th>\n      <th>heals</th>\n      <th>kill_place</th>\n      <th>kill_points</th>\n      <th>kills</th>\n      <th>kill_streaks</th>\n      <th>longest_kill</th>\n      <th>match_duration</th>\n      <th>match_type</th>\n      <th>max_place</th>\n      <th>num_groups</th>\n      <th>rank_points</th>\n      <th>revives</th>\n      <th>ride_distance</th>\n      <th>road_kills</th>\n      <th>swim_distance</th>\n      <th>team_kills</th>\n      <th>vehicle_destroys</th>\n      <th>walk_distance</th>\n      <th>weapons_acquired</th>\n      <th>win_points</th>\n      <th>win_place_perc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7f96b2f878858a</td>\n      <td>4d4b580de459be</td>\n      <td>a10357fd1a4a91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>60</td>\n      <td>1241</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>1306</td>\n      <td>squad-fpp</td>\n      <td>28</td>\n      <td>26</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>244.80</td>\n      <td>1</td>\n      <td>1466</td>\n      <td>0.4444</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>eef90569b9d03c</td>\n      <td>684d5656442f9e</td>\n      <td>aeb375fc57110c</td>\n      <td>0</td>\n      <td>0</td>\n      <td>91.47</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>1777</td>\n      <td>squad-fpp</td>\n      <td>26</td>\n      <td>25</td>\n      <td>1484</td>\n      <td>0</td>\n      <td>0.0045</td>\n      <td>0</td>\n      <td>11.04</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1434.00</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0.6400</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1eaf90ac73de72</td>\n      <td>6a4a42c3245a74</td>\n      <td>110163d8bb94ae</td>\n      <td>1</td>\n      <td>0</td>\n      <td>68.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>47</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>1318</td>\n      <td>duo</td>\n      <td>50</td>\n      <td>47</td>\n      <td>1491</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>161.80</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.7755</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4616d365dd2853</td>\n      <td>a930a9c79cd721</td>\n      <td>f1f1f4ef412d7e</td>\n      <td>0</td>\n      <td>0</td>\n      <td>32.90</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>75</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>1436</td>\n      <td>squad-fpp</td>\n      <td>31</td>\n      <td>30</td>\n      <td>1408</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>202.70</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0.1667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>315c96c26c9aac</td>\n      <td>de04010b3458dd</td>\n      <td>6dc8ff871e21e6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>100.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>45</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>58.53</td>\n      <td>1424</td>\n      <td>solo-fpp</td>\n      <td>97</td>\n      <td>95</td>\n      <td>1560</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>49.75</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.1875</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "(100000, 29)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['id',\n 'group_id',\n 'match_id',\n 'assists',\n 'boosts',\n 'damage_dealt',\n 'dbn_os',\n 'headshot_kills',\n 'heals',\n 'kill_place',\n 'kill_points',\n 'kills',\n 'kill_streaks',\n 'longest_kill',\n 'match_duration',\n 'match_type',\n 'max_place',\n 'num_groups',\n 'rank_points',\n 'revives',\n 'ride_distance',\n 'road_kills',\n 'swim_distance',\n 'team_kills',\n 'vehicle_destroys',\n 'walk_distance',\n 'weapons_acquired',\n 'win_points',\n 'win_place_perc']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "3de135c791d59e5c3abcc80d762553e0bed92493"
      },
      "cell_type": "markdown",
      "source": "### Drop NaN Target"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "101bd61df28ab6dc6f499ad08c3059ff51078f48"
      },
      "cell_type": "code",
      "source": "df.drop(df[df['win_place_perc'].isnull()].index, inplace=True)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ce97ee3abf4e0cc6cf46345ad3dc300c7b1b5895"
      },
      "cell_type": "markdown",
      "source": "### Select Numerical Features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f976ff196c9d74fdb54a00ce170f108db9335dea"
      },
      "cell_type": "code",
      "source": "id_features = ['id', 'group_id', 'match_id']\ncategorical_features = ['match_type', ]\ntarget_feature = 'win_place_perc'\nbase_features = [col for col in df.columns if col not in id_features + categorical_features + [target_feature]] ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ce90b6719c541cc73732273c77ed2f05d13f718b"
      },
      "cell_type": "markdown",
      "source": "### Train 1"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c74b0684d6fad5060923f5c9e57b749ddef318a"
      },
      "cell_type": "code",
      "source": "%%time\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\n\nkfold = KFold(n_splits = 2)\nkfold.get_n_splits(df)\nlog = []\nfor train_index, valid_index in kfold.split(df):\n    step = dict()\n    pipeline = Pipeline(\n        id_columns=id_features, \n        numerical_columns=base_features,\n        categorical_columns=categorical_features,\n        target_column=target_feature,\n    )\n    x_train = pipeline.fit_transform(df.loc[train_index, :])\n    y_train = df.loc[train_index, target_feature]\n    \n    print('Fitting ...')\n#     model = Ridge()\n    model = RandomForestRegressor(\n        n_jobs=-1, \n        n_estimators=20, \n        criterion='mae', \n        max_depth=10,\n        verbose=2,\n    )\n    model.fit(x_train, y_train)\n    step['train_score'] = mean_absolute_error(y_train, model.predict(x_train))\n#     del x_train, y_train\n    \n    x_valid = pipeline.transform(df.loc[valid_index, :])\n    y_valid = df.loc[valid_index, target_feature]\n    \n    step['valid_score'] = mean_absolute_error(y_valid, model.predict(x_valid))\n    step['model'] = model\n    step['pipeline'] = pipeline\n    step['train_index'] = train_index\n    step['valid_index'] = valid_index\n    try:\n        save_model(step)\n    except Exception:\n        print(\"Warning: Couldn't save the model\")\n    print(step['train_score'], step['valid_score'])\n    log.append(step)\n    break",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Transforming ...\nFeatureGenerator ...\nPreprocessor ...\nFitting ...\nbuilding tree 1 of 20building tree 2 of 20\nbuilding tree 3 of 20\n\nbuilding tree 4 of 20\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "building tree 5 of 20\nbuilding tree 6 of 20\nbuilding tree 7 of 20\nbuilding tree 8 of 20\nbuilding tree 9 of 20\nbuilding tree 10 of 20\nbuilding tree 11 of 20\nbuilding tree 12 of 20\nbuilding tree 13 of 20\nbuilding tree 14 of 20\nbuilding tree 15 of 20\nbuilding tree 16 of 20\nbuilding tree 17 of 20\nbuilding tree 18 of 20\nbuilding tree 19 of 20\nbuilding tree 20 of 20\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 20.5min finished\n[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Transforming ...\nFeatureGenerator ...\nPreprocessor ...\nWarning: Couldn't save the model\n0.06083124505000001 0.06916155065\nCPU times: user 1h 21min 15s, sys: 2.22 s, total: 1h 21min 17s\nWall time: 20min 35s\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "857837c6124fcaee2dda1afdbb18a8a06181ada8"
      },
      "cell_type": "markdown",
      "source": "### Submission"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6343b903d6f30056791ed04ca1c36a162366c55c"
      },
      "cell_type": "code",
      "source": "df_test = pd.read_csv('../input/test_V2.csv')\ndf_test.columns = [camelcase_to_underscore(col) for col in df_test.columns]",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49ddb860bec139fb4b0415628d83ad3e312c5f7f"
      },
      "cell_type": "code",
      "source": "df_test = pipeline.transform(df_test)",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Transforming ...\nFeatureGenerator ...\nPreprocessor ...\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2ff446657c00a725c89edb3304cd17930432410"
      },
      "cell_type": "code",
      "source": "pred = model.predict(df_test)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    2.1s finished\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24d1992032a36575b2404e815aa708abed4011b8"
      },
      "cell_type": "code",
      "source": "df_sub = pd.read_csv('../input/sample_submission_V2.csv')\ndf_sub['winPlacePerc'] = pred\ndf_sub.to_csv('bad_submission.csv', index=False)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e03f29f8c36e765d204cc9eb4281403b9d525ec"
      },
      "cell_type": "markdown",
      "source": "21-40 - start"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed9267d9f315e0e072fce4c56b94167d38a300c5"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}